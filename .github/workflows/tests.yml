name: Tests

on:
  pull_request:
    branches: [ main, master ]
  push:
    branches: [ main, master ]

jobs:
  test:
    runs-on: ubuntu-latest

    steps:
    - uses: actions/checkout@v3

    - name: Set up Python 3.10
      uses: actions/setup-python@v4
      with:
        python-version: '3.10'

    - name: Cache pip packages
      uses: actions/cache@v3
      with:
        path: ~/.cache/pip
        key: ${{ runner.os }}-pip-${{ hashFiles('**/requirements*.txt') }}
        restore-keys: |
          ${{ runner.os }}-pip-

    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt
        pip install -r requirements-dev.txt

    - name: Cache benchmark files
      uses: actions/cache@v3
      with:
        path: ./benchmarks/cache
        key: ${{ runner.os }}-benchmarks-${{ hashFiles('benchmarks/downloaders/**/*.py') }}
        restore-keys: |
          ${{ runner.os }}-benchmarks-

    - name: Download curated benchmarks
      run: |
        # Download and create ALL curated benchmark sets (cached by GitHub Actions)
        # This creates all 4 curated sets: SL-COMP, QF_S, QF_AX, QF_BV
        python -m benchmarks download --curated

    - name: Run regression tests
      run: |
        python -m pytest tests/ -v --tb=short

    - name: Test import
      run: |
        python -c "from frame import EntailmentChecker; print('Import successful')"

    - name: Quick functionality test
      run: |
        python -c "
        from frame import EntailmentChecker
        checker = EntailmentChecker()
        result = checker.check_entailment('x |-> 5 |- x |-> 5')
        assert result.valid, 'Basic entailment check failed'
        print('✓ Basic functionality test passed')
        "

    - name: Run curated benchmarks (sample)
      run: |
        # Run FULL curated benchmark sets (all 4,742 tests)
        # - 692 SL-COMP (separation logic)
        # - 3,300 QF_S (string theory)
        # - 500 QF_AX (array theory)
        # - 250 QF_BV (bitvector theory)
        python -m benchmarks run --curated --output ci_results.json

    - name: Check benchmark results
      run: |
        python -c "
        import json
        with open('ci_results.json') as f:
            results = json.load(f)
        correct = sum(1 for r in results if r['expected'] == r['actual'])
        total = len(results)
        accuracy = correct / total * 100 if total > 0 else 0
        print(f'Benchmark Results: {correct}/{total} correct ({accuracy:.1f}%)')
        # Fail if accuracy drops below 80% (regression detection)
        assert accuracy >= 80, f'Accuracy too low: {accuracy:.1f}% < 80%'
        print('✓ Benchmark accuracy check passed')
        "
