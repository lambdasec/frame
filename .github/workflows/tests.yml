name: Tests

on:
  pull_request:
    branches: [ main, master ]
  push:
    branches: [ main, master ]

jobs:
  test:
    runs-on: ubuntu-latest

    steps:
    - uses: actions/checkout@v3

    - name: Set up Python 3.10
      uses: actions/setup-python@v4
      with:
        python-version: '3.10'

    - name: Cache pip packages
      uses: actions/cache@v3
      with:
        path: ~/.cache/pip
        key: ${{ runner.os }}-pip-${{ hashFiles('**/requirements*.txt') }}
        restore-keys: |
          ${{ runner.os }}-pip-

    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt
        pip install -r requirements-dev.txt

    - name: Cache benchmark files
      uses: actions/cache@v3
      with:
        path: ./benchmarks/cache
        key: ${{ runner.os }}-benchmarks-${{ hashFiles('benchmarks/downloaders/**/*.py') }}
        restore-keys: |
          ${{ runner.os }}-benchmarks-

    - name: Download curated benchmarks
      run: |
        # Download and create curated benchmark sets (cached by GitHub Actions)
        python -c "
        from benchmarks.curators import create_qf_ax_curated_set, create_qf_bv_curated_set
        import os

        cache_dir = './benchmarks/cache'
        os.makedirs(cache_dir, exist_ok=True)

        # Create QF_AX curated set
        if not os.path.exists(os.path.join(cache_dir, 'qf_ax', 'qf_ax_curated')):
            print('Creating QF_AX curated set...')
            create_qf_ax_curated_set(cache_dir)

        # Create QF_BV curated set
        if not os.path.exists(os.path.join(cache_dir, 'qf_bv', 'qf_bv_curated')):
            print('Creating QF_BV curated set...')
            create_qf_bv_curated_set(cache_dir)

        print('✓ Curated benchmarks ready')
        "

    - name: Run regression tests
      run: |
        python -m pytest tests/ -v --tb=short

    - name: Test import
      run: |
        python -c "from frame import EntailmentChecker; print('Import successful')"

    - name: Quick functionality test
      run: |
        python -c "
        from frame import EntailmentChecker
        checker = EntailmentChecker()
        result = checker.check_entailment('x |-> 5 |- x |-> 5')
        assert result.valid, 'Basic entailment check failed'
        print('✓ Basic functionality test passed')
        "

    - name: Run curated benchmarks (sample)
      run: |
        # Run full curated benchmark sets (QF_AX and QF_BV)
        # Note: QF_S (string theory) is skipped due to poor Z3 support (~8% accuracy)
        # Note: SL-COMP requires large downloads, skipped in CI
        python -m benchmarks run --division qf_ax_curated --output ci_results_ax.json
        python -m benchmarks run --division qf_bv_curated --output ci_results_bv.json

        # Combine results into single file
        python -c "
        import json
        ax = json.load(open('ci_results_ax.json'))
        bv = json.load(open('ci_results_bv.json'))
        combined = ax + bv
        json.dump(combined, open('ci_results.json', 'w'), indent=2)
        print(f'Combined {len(ax)} QF_AX + {len(bv)} QF_BV = {len(combined)} total tests')
        "

    - name: Check benchmark results
      run: |
        python -c "
        import json
        with open('ci_results.json') as f:
            results = json.load(f)
        correct = sum(1 for r in results if r['expected'] == r['actual'])
        total = len(results)
        accuracy = correct / total * 100 if total > 0 else 0
        print(f'Benchmark Results: {correct}/{total} correct ({accuracy:.1f}%)')
        # Fail if accuracy drops below 80% (regression detection)
        assert accuracy >= 80, f'Accuracy too low: {accuracy:.1f}% < 80%'
        print('✓ Benchmark accuracy check passed')
        "
